%&program=xelatex
\TeXXeTstate=1
\documentclass[letterpaper,11pt]{article}
\flushbottom

\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\textfraction}{0.0}

\setlength{\textwidth}{6in}
\setlength{\textheight}{8.5in}
\oddsidemargin=0.25in
\topmargin=0in

\ifx\XeTeXversion\undefined
\def\XeTeX{\leavevmode
  \setbox0=\hbox{X\lower.5ex\hbox{\kern-.15em\hbox{E}}\kern-.1667em \TeX}%
  \dp0=0pt\ht0=0pt\box0 }
\else
\usepackage{euler,fontspec}
\defaultfontfeatures{Mapping=tex-text}
\setromanfont{Adobe Garamond Pro}
\setmonofont[Scale=0.8]{Andale Mono WT J}

% Define the \XeTeX logo:
\def\reflect#1{{\setbox0=\hbox{#1}\rlap{\kern0.5\wd0
  \special{x:gsave}\special{x:scale -1 1}}\box0 \special{x:grestore}}}
\def\XeTeX{\leavevmode
  \setbox0=\hbox{X\lower.5ex\hbox{\kern-.15em\reflect{E}}\kern-.1667em \TeX}%
  \dp0=0pt\ht0=0pt\box0 }
\fi

\def\eTeX{$\varepsilon$-\TeX}
\def\TeXgX{\TeX\lower.5ex\hbox{\kern-.15em G}\kern-.1667em X}

\usepackage[format=hang,font=small,labelfont=bf]{caption}

\usepackage{fancyhdr,multicol,url}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\headheight}{14pt}
\fancyhf{}
\fancyhead[C]{\small The Multilingual Lion: \TeX\ learns to speak Unicode}
\fancyfoot[L]{\footnotesize 27th Internationalization and Unicode Conference}
\fancyfoot[C]{\footnotesize \thepage}
\fancyfoot[R]{\footnotesize Berlin, Germany, April 2005}

\frenchspacing
\catcode`\_=12

\def\texicon{\hbox to 0pt{\kern1.8cm\vbox to 0pt{\kern-1cm
  \XeTeXpicfile "tex-icon.tiff"\vss}\hss}}

\title{The Multilingual Lion:\texicon\footnote{Why a multilingual {\em lion}?
Because \TeX’s logo is a lion;
see Knuth’s {\em The \TeX book} (Addison Wesley: 1984) or other sources.}\\
\TeX\ learns to speak Unicode}

\author{Jonathan Kew\\SIL International}

\begin{document}
\maketitle
\thispagestyle{fancy}

\pretolerance=10000

\begin{abstract}
Professor Donald Knuth’s \TeX\ is a typesetting system with a wide user community, and a range of supporting packages and enhancements available for many types of publishing work. However, it dates back to the 1980s and is tightly wedded to 8-bit character data and custom-encoded fonts, making it difficult to configure \TeX\ for many complex-script languages.

This paper will focus on \XeTeX, a system that extends \TeX\ with direct support for modern OpenType and AAT fonts and the Unicode character set. This makes it possible to typeset almost any script and language with the same power and flexibility as \TeX\ has traditionally offered in the 8-bit, simple-script world of European languages. \XeTeX\ (currently available on Mac OS X, but possibly on other platforms in the future) integrates the \TeX\ formatting engine with technologies from both the host operating system (Apple Type Services, Text Encoding Converter) and auxiliary libraries (ICU, TECkit). Thus, it illustrates how such components can be leveraged to provide the benefits of Unicode within an existing software system.

This paper should be of interest to those involved in multilingual and multiscript publishing, as well as developers seeking to enhance legacy systems to take advantage of the benefits of Unicode. The merger of legacy and Unicode-based technologies means that the benefits of many years of development in the \TeX\ world become available for document production in a much wider range of languages.

Some background familiarity with \TeX\ may be helpful, but the paper’s focus will be on the integration of Unicode technologies, not on technical details of \TeX\ itself. A general awareness of encodings, complex scripts, and font technologies will be assumed.
\end{abstract}

\section{Background}
The \TeX\ typesetting system has a 20-year history as a stable and reliable tool for producing well-formatted documents from marked-up source text,
and offers a great deal of power, flexibility and extensibility by virtue of a powerful macro language.
The extensive user community, especially in the academic world,
has created a large collection of supporting packages for many different types of document.

For those unfamiliar with \TeX, a brief overview may be helpful. The \TeX\ processor reads a source document, recognizing characters as either text to be typeset or markup according to scanning rules and (customizable) character categories. It expands macros and executes commands (setting parameters to control the typesetting process, for example), and forms the text into paragraphs and pages. Finally, a compact representation of the typeset pages is written out to a DVI (“device independent”) file; a subsequent device driver process reads this \verb|.dvi| file and renders the pages to a specific destination such as a screen or printer.
(In the case of \XeTeX, to be discussed below, this output format has been extended and renamed XDV, and the default behavior is to automatically run an XDV-to-PDF processor, so that the effective output format is PDF.)

Over the years, \TeX\ has been used with many non-English languages,
often using combinations of custom-encoded 8-bit fonts and different input encodings and conventions.
However, \TeX’s roots are unquestionably in Latin-script typography;
the system originally processed 7-bit text (usually ASCII), accessing 8-bit fonts for output.
Version~3 extended the system to support 8-bit input text,
and provided some enhancements for multilingual use,
but support for many non-Latin and complex scripts remains a problem.

In addition to standard 8-bit codepages, there are ways of using \TeX’s programmability
to allow input of additional text elements as by representing them as character sequences.
Some conventions are so widely used that many users think of them as a standard part of the \TeX\ program (though this is not really the case); others are associated with macro packages for particular languages; and still others are created just for specific projects.
Figure~\ref{fig-inputconventions} shows a few examples of typical \TeX\ input conventions for non-ASCII characters.

\font\dn="Devanagari MT" at 11pt
\begin{figure}[tb]
\hrule\smallskip
\begin{tabular}{ccl}
\em Source text&\em Typeset result&\em Notes\\
\verb|\'{a}|&\'a&\verb|\'| is one of various commands to add an accent to a letter\\
\verb|\c{c}|&\c c&an accent that attaches below the letter\\
\verb|\aa|&\aa&special command for a specific character\\
\verb|---|&---&implemented as a ligature in standard \TeX\ fonts\\
\verb|\alpha|&$\alpha$&one of many symbols available in {\em math mode}\\
\verb|{\dn acchaa}|&\dn अच्छा&requires use of a special preprocessor and custom fonts\\
\end{tabular}
\smallskip\hrule
\caption{Traditional \TeX\ input conventions for non-ASCII characters in 7-bit source text.}
\label{fig-inputconventions}
\end{figure}

While these conventions can be extended almost indefinitely, they tend to clutter the source text; and they rely on a variety of custom-encoded fonts to provide all the symbols needed.
Unicode offers the possibility of a far simpler model for typesetting multilingual text, where each character needed is represented in the source not by some sequence of commands, but as itself.

In the case of complex scripts such as Devanagari, solutions based on standard \TeX\ typically involve a custom preprocessor that performs the contextual analysis needed for proper rendering of the script, starting from some (often romanized) input convention, and emits special \TeX\ commands to access the appropriate glyphs from custom 8-bit fonts.
While such solutions can work, they may be complex to use, and fragile in how they interact with various other macro packages for document formatting.
And trying to combine several such solutions to create a highly multilingual document, using several complex scripts simultaneously, goes far beyond what typical users can be expected to achieve.

To address these issues, an extended version of \TeX\ known as \XeTeX\ has been developed. This is a Unicode-based multilingual typesetting system that works with existing “smart font” technologies to provide complex script support, within the framework of the formatting power, flexibility, and programmability of \TeX.


\section{Examples of use}

Before looking at what was involved in extending \TeX\ to support Unicode and smart font technologies for text rendering, I will show a few brief examples of \XeTeX\ at work. Figures~\ref{fig-sorting-a} to \ref{fig-scripture} illustrate how readily Unicode text now fits into the \TeX\ typesetting paradigm. In each case, the “raw” source (text and markup) is shown alongside the typeset result.

\input{figure-sorting-a}
\input{figure-sorting-b}
\input{figure-biblio}
\input{figure-scripture}

\section{Extending the character set}

The first step towards Unicode support in \TeX\ is to expand the character set
beyond the original 256-character limit.
At the lowest level, this means changing internal data structures throughout,
wherever characters were stored as 8-bit values.
As Unicode scalar values may be up to U+10FFFF,
an obvious modification would be to make “characters” 32~bits wide,
and treat Unicode characters as the basic units of text.

However, in \XeTeX\ a pragmatic decision was made to work internally with UTF-16
as the encoding form of Unicode,
making “characters” in the engine 16~bits wide,
and handling supplementary-plane characters using UTF-16 surrogate pairs.
This choice was made for a number of reasons:

\begin{itemize}

\item The operating-system APIs that \XeTeX\ expects to use in working with Unicode text require UTF-16, so working with this encoding form avoids the need for conversion at this interface.

\item There are a number of internal tables in the \TeX\ program that are implemented as arrays indexed by character code. In standard \TeX, these arrays have 256~elements each.
Enlarging them to 65,536 elements each, to index them by UTF-16 code values, is just about reasonable; enlarging them further, to allow direct indexing by Unicode scalar values, would make for extremely large arrays.
To keep the memory footprint reasonable (both at runtime and for “dumped” macro collections), some kind of sparse array implementation would probably be needed, requiring significant additional development and testing, and perhaps impacting performance of key inner-loop parts of the \TeX\ system.

\item These per-character arrays are used to implement character “categories”, used in parsing input text into tokens, as well as case conversions and “space factor” (a property used to modify word spacing for punctuation in Roman typography).
In practice, it seems unlikely that there will be a great need to customize these character properties for individual supplementary-plane characters.
They're unlikely to be wanted as escape characters or other special categories of \TeX\ input;
need not have the “letter” property that allows them to be part of \TeX\ control sequences;
and probably don't need to be included in automatic hyphenation patterns.

\end{itemize}

In view of these factors, \XeTeX\ works with UTF-16 code units, and Unicode characters beyond U+FFFF cannot be given individually-customized \TeX\ properties.
They can still be included in documents, however, and will render correctly (given appropriate fonts) as the UTF-16 surrogate pairs will be properly passed to the font system.

Another possible route would have been to use UTF-8 as the internal encoding form, retaining the existing 8-bit code units used in \TeX\ as characters.
However, this would have made it impossible (without major revisions) to provide properties such as character category (letter, other printing character, escape, grouping delimiter, comment character, etc.), case mappings, and so on to any characters beyond the basic ASCII set; and it would also require conversion when Unicode text is to be passed to system APIs.
Overall, therefore, UTF-16 was felt to be the most practical choice, and the appropriate \TeX\ data structures were systematically widened.

\section{Implementing a character/glyph model}

An important aspect of rendering Unicode text is the character/glyph model; it is assumed that the reader is familiar with this concept.
Traditionally, \TeX\ does not have a well-developed character/glyph model. Input text is a sequence of 8-bit codes, interpreted as character tokens or other (e.g., control sequence) tokens according to the scanning rules and character categories.
These same 8-bit codes are used as access codes for glyphs in fonts.
It is possible to remap codes by \TeX\ macro programming,
and the “font metrics” (\verb|.tfm|) files used by \TeX\ can include simple ligature rules
(e.g., \verb|fi| $\mapsto$ fi), but the model is fairly rudimentary, and not adequate for script behaviors such as Arabic cursive shaping or Indic reordering.
To support the full range of complex scripts in Unicode, a more complete character/glyph model is needed.

Rather than designing a text rendering system based on the Unicode character/glyph model from scratch, it seemed desirable to leverage existing implementations,
allowing \TeX\ to take advantage of the “smart fonts” and multilingual text rendering facilities found in modern operating systems and libraries.
At the time of writing, \XeTeX\ supports two such rendering systems; it is possible that additional ones will be supported in future versions.

\subsection{Using ATSUI on Mac OS X}

The first smart-font rendering system implemented in \XeTeX\ was the ATSUI\footnote{Apple Type Services for Unicode Imaging; see \url{http://developer.apple.com/intl/atsui.html}.} system under Mac OS X.
ATSUI is the Mac OS X component that renders Unicode text using AAT\footnote{Apple Advanced Typography; see \url{http://developer.apple.com/fonts/TTRefMan/RM06/Chap6AATIntro.html}.} fonts.
The essential objects needed to render text with ATSUI are {\em text layouts} and associated {\em styles} (which in turn refer to {\em fonts} and other attributes).

In order for a system like ATSUI to render text correctly, it must be given complete runs of text, not individual characters; otherwise, behavior such as reordering and contextual glyph selection cannot happen. \TeX\ normally treats each character of text as an individual node in a list, with known (and fixed) dimensions. Paragraph layout consists of taking a list of such nodes, with intervening “glue” (potentially flexible space) and other items, and determining the best sequence of line-break positions and the final location of each character and other node.

When using ATSUI for Unicode text, however, \XeTeX\ cannot treat each character (or, strictly speaking, UTF-16 code unit) as a separate node, to be measured and positioned individually.
Instead, it collects sequences of characters that share the same font style, and calls ATSUI to measure such sequences (typically, entire words). A paragraph then consists of a list of such “word nodes”, each with its dimensions as determined by ATSUI, and intervening space and other nodes. The basic \TeX\ paragraphing algorithm applies just as well to these larger “chunks” as to traditional character nodes.

During formatting, then, \XeTeX\ makes use of just a few basic ATSUI APIs, in order to measure each word (or similar fragment) of text; in particular:

\begin{description}

\item[ATSUCreateStyle, ATSUSetAttributes] Create an ATSUI style object, and assign appropriate text attributes.
One ATSUStyle is associated with each font face and size combination requested by the \TeX\ document, and used whenever text in that particular style needs to be measured.

\item[ATSUCreateTextLayout, ATSUSetTextPointerLocation, ATSUSetRunStyle] Create an ATSUI text layout object, and associate a string of Unicode text and a style object with it.

\item[ATSUGetUnjustifiedBounds] Measure a range of text as rendered with the associated font and other attributes. This gives the \TeX\ paragraphing algorithm the measurements that it will use in laying out the text.

\end{description}
(In addition, a number of font-related ATSUI APIs are used to enumerate the fonts available in the system, determine what layout features the fonts support, etc.)

When \XeTeX\ has completed layout for a paragraph of text, therefore, it has a list of lines each containing a list of “word nodes”; each such node contains a run of Unicode text and a reference to an ATSUI style. The \TeX\ system {\em does not know} the details of the actual glyphs that will be used to render the text, or precisely where they will be positioned; only the overall dimensions and position of each word. The glyph-level detail is left entirely to the ATSUI rendering system.

To illustrate this, figure~\ref{fig-showbox} shows the trace output generated for a short fragment of text set as a single-line paragraph. Using a standard \TeX\ font, the line consists of a list of character nodes, with intervening “glue” (flexible space). Each character has an associated font, and its metrics will be looked up in the corresponding \TeX\ font metric (TFM) file. Note that \TeX\ has automatically inserted some kerns between adjacent letters; this is also controlled by the TFM file for this font. In contrast, using an AAT font, the text line consists simply of a list of complete words with intervening glue; \TeX\ does not deal with the individual characters. There may be kerning between letters here too, but the \TeX\ algorithms are unaware of it; it happens automatically as ATSUI measures or renders the words, and \TeX\ knows only their overall dimensions.

\begin{figure}[htb]
\hrule\smallskip
\begin{minipage}[t]{0.5\hsize}
\small
\begin{verbatim}
\font\AATfont="Times Roman" \AATfont
\setbox0=\vbox{The quick brown fox.}
\showbox0

\font\TeXfont=cmr10 \TeXfont
\setbox1=\vbox{The quick brown fox.}
\showbox1
\end{verbatim}

\footnotesize
\begin{verbatim}
> \box0=
\vbox(8.0+2.0)x469.75499
.\hbox(8.0+2.0)x469.75499, glue set 363.10948fil
..\hbox(0.0+0.0)x20.0
..\AATfont The
..\glue 2.5 plus 1.66666 minus 0.83333
..\AATfont quick
..\glue 2.5 plus 1.66666 minus 0.83333
..\AATfont brown
..\glue 2.5 plus 1.66666 minus 0.83333
..\AATfont fox.
..\penalty 10000
..\glue(\parfillskip) 0.0 plus 1.0fil
..\glue(\rightskip) 0.0
\end{verbatim}
\end{minipage}\hfil
\begin{minipage}[t]{0.5\hsize}
\footnotesize
\begin{verbatim}
> \box1=
\vbox(6.94444+1.94444)x469.75499
.\hbox(6.94444+1.94444)x469.75499, glue set 356.6715fil
..\hbox(0.0+0.0)x20.0
..\TeXfont T
..\TeXfont h
..\TeXfont e
..\glue 3.33333 plus 1.66666 minus 1.11111
..\TeXfont q
..\TeXfont u
..\TeXfont i
..\TeXfont c
..\kern-0.27779
..\TeXfont k
..\glue 3.33333 plus 1.66666 minus 1.11111
..\TeXfont b
..\TeXfont r
..\TeXfont o
..\kern-0.27779
..\TeXfont w
..\TeXfont n
..\glue 3.33333 plus 1.66666 minus 1.11111
..\TeXfont f
..\TeXfont o
..\kern-0.27779
..\TeXfont x
..\TeXfont .
..\penalty 10000
..\glue(\parfillskip) 0.0 plus 1.0fil
..\glue(\rightskip) 0.0
\end{verbatim}
\end{minipage}
\smallskip\hrule
\caption{Using tracing commands to examine \XeTeX's internal structures for text using an AAT font, compared to the same text with a standard \TeX\ font.}
\label{fig-showbox}
\end{figure}

After document formatting is complete, the \XeTeX\ “back-end” (actually a separate process, {\tt xdv2pdf}) reads the \verb|.xdv| file that encodes the formatted document, and creates a PDF version for viewing or printing.
To do this, it “renders” the page encoded in the \verb|.xdv| file through the Mac OS X Quartz graphics system, with a PDF file as the rendering destination.
At this point, it again uses ATSUI APIs, loading each text string into an ATSUTextLayout, assigning the proper style, and calling {\bf ATSUDrawText} to image the text into the PDF being constructed.

\subsection{Using OpenType via ICU Layout}

While the initial implementation of \XeTeX\ was based on Apple's ATSUI rendering system, the increasing availability of fonts with OpenType layout features led to a desire to also support this font technology.
The system was therefore extended by incorporating the OpenType layout engine from ICU\footnote{IBM's open-source project, International Components for Unicode; see \url{http://oss.software.ibm.com/icu/}.}. (In addition to the actual layout engine, \XeTeX\ makes use of ICU's implementation of the Unicode Bidi Algorithm.) The main functions used in the typesetting process include:

\begin{description}
\item[ubidi_open, ubidi_close, ubidi_setPara, ubidi_getDirection, ubidi_countRuns, ubidi_getVisualRun] Before laying out glyphs, it is necessary to deal with bidirectional layout issues; most “chunks” \XeTeX\ needs to measure will be unidirectional, but this is not always the case. With mixed-direction text, each direction run is measured separately.
\item[LayoutEngine::layoutChars, getGlyphs, getGlyphPositions] The ICU LayoutEngine class is used to perform the actual layout process, and retrieve the list of glyphs and positions. The resulting array of positioned glyphs is stored within the “word node” in \XeTeX's paragraph list.
\end{description}

Internally, ICU-based OpenType rendering is handled in a very different way from ATSUI rendering. With ATSUI, the output of the typesetting process includes the original Unicode strings and the appropriate font descriptors; the PDF-generating back-end then reuses ATSUI layout functions to actually render the text into the PDF destination. In the case of OpenType, however, the typesetting process retrieves the array of positioned glyphs that result from the layout operation, and records this; the back-end then merely has to draw the glyphs as specified, not repeat any of the text layout work.

When the \TeX\ source calls for a particular font, \XeTeX\ looks for specific layout tables within the font (e.g., \verb|morx| for AAT, or \verb|GSUB| for OpenType) to determine which layout engine to use, and instantiates either an ATSUI style or an ICU LayoutEngine as appropriate.
The difference in the implementation of the two technologies is, however, entirely hidden from the main \TeX\ program, which simply deals with “word nodes”, forming them into paragraphs and pages once they've been measured by the appropriate smart-font engine.

\subsection{Hyphenation support}

Implementing “word nodes” as “black boxes” within the main \TeX\ program made it easy to form paragraphs of such words, without extensive changes to the rest of \TeX.
A complication arose, however, in that \TeX\ has an automatic hyphenation algorithm that comes into effect if it is unable to find satisfactory line-break positions for a paragraph.
The hyphenation routine applies to lists of character nodes representing runs of text within a paragraph to be line-broken. But at this level, the program sees Unicode “word nodes” as indivisible, rigid chunks.

Explicit discretionary hyphens may be included in \TeX\ input, and these continue to work in \XeTeX, as they become “discretionary break” nodes in the list of items making up the paragraph. The word fragments on either side, then, would become separate nodes in the list, and a line-break can occur between them. 

In order to reinstate hyphenation support, therefore, it was necessary to extend the hyphenation routine so as to be able to extract the text from a word node, use \TeX's pattern-based algorithm to find possible hyphenation positions within the word, and then replace the original word node with a sequence of nodes representing the (possibly) hyphenated fragments, with discretionary hyphen nodes in between.

A final refinement proved necessary here: once the line-breaks have been chosen, and the lines of text are being “packaged” for justification to the desired width, any unused hyphenation points are removed and the adjacent word (fragment) nodes re-merged. This is required in order to allow rendering behavior such as character reordering and ligatures, implemented at the smart-font level, to occur across hyphenation points. With an early release of \XeTeX, a user reported that OpenType ligatures in certain words such as {\em different} would intermittently fail (appearing as {\em dif\kern0pt ferent} instead), and this turned out to be caused when automatic hyphenation came into effect and a discretionary break was inserted.

\section{Backward compatibility}

The original motivation for the \XeTeX\ project was to provide a typesetting solution
that worked with Unicode and complex scripts, via smart font technologies.
However, it soon became clear that many existing \TeX\ users,
with no complex-script requirements,
nevertheless found the integration with the host platform’s font management
to be very attractive, and wished to use \XeTeX\ and native Mac OS X fonts
with existing \TeX\ (or more commonly \LaTeX) documents.
There is a huge legacy of pre-Unicode \TeX\ documents and resources, and it is helpful for users to be able to continue working with these materials, while at the same time beginning to take advantage of the extended capabilities of \XeTeX.

\subsection{Traditional \TeX\ input conventions}

Existing \TeX\ and \LaTeX\ documents often use ASCII-based sequences to represent accented and other “extended” characters not directly available in the input character set. The macro packages that implement these commands map them to known character codes in particular font encodings.

To allow such documents to be typeset using standard Unicode-compliant fonts in place of the custom-encoded fonts previously used, Ross Moore (an early \XeTeX\ user) has provided a package\footnote{See the {\em utf8accents} package, available from \url{http://scripts.sil.org/xetex_related}.} for \LaTeX\ that maps several hundred such control sequences to the correct Unicode codepoints. Using this package, many existing \LaTeX\ documents that use extended Latin and other “special” characters can be typeset using Unicode fonts, without needing to actually convert the encoding of the source text.

\subsection{Legacy source document encodings}

As initially designed, \XeTeX\ assumed that all input text is encoded in Unicode; it would read input files as either UTF-8 or UTF-16. Existing ASCII documents, of course, are also valid UTF-8 and therefore could be used directly. This includes documents that use ASCII-based \TeX\ conventions for accents and other extended characters, as mentioned above.

However, some \TeX\ users have documents encoded with 8-bit codepages such as ISO Latin-1, MacRoman, Windows Cyrillic, etc.
With the original “pure Unicode” implementation of \XeTeX, it was impossible to process such files; they would be assumed to be UTF-8, but on encountering values $>127$, the bytes would be misinterpreted as UTF-8 sequences rather than as individual characters. (In standard \TeX, with purely byte-oriented input, such files can of course be read; and the characters can be remapped through \TeX\ macro programming, if (as often occurs) there is a mismatch between the encodings of input text and the fonts to be used.

To enable users to process such files with \XeTeX, without requiring a separate conversion to Unicode first, more flexible input encoding support was eventually added to the system. A new command \verb|\XeTeXinputencoding| was implemented, which allows the user to request on-the-fly conversion from another character encoding into Unicode as the source text is read.

The \verb|\XeTeXinputencoding| command requires one parameter, the name of the desired encoding. A number of options are supported. First, \verb|utf8| or \verb|utf16| will set the system to direct Unicode input (or \verb|auto| restores the default behavior, which is to detect the Unicode encoding form from the initial bytes of the file). The special name \verb|bytes| causes \XeTeX\ to read individual byte values as separate code units, treating them as character codes 0–255. While this is unlikely to represent the correct Unicode interpretation of the source text, it may be useful if these codes are to be processed by existing \TeX\ macros rather than used directly as text characters.

Finally, any Internet encoding name known to the Mac OS X Text Encoding Converter\footnote{A standard component of the Mac OS; see \url{http://developer.apple.com/documentation/Carbon/Reference/Text_Encodin_sion_Manager/index.html}.} may be specified.
In this case, \XeTeX\ calls TEC to perform encoding conversion as it reads the input text.
Just a few basic TEC APIs are sufficient for this task:
\begin{description}
\item[TECGetTextEncodingFromInternetName] Used by \verb|\XeTeXinputencoding| to look up the encoding name specified, and determine if it is known to the operating system.
\item[CreateTextToUnicodeInfo] Initialize the mapping information needed by TEC to convert between a particular legacy encoding and Unicode.
\item[ConvertFromTextToUnicode] Convert a buffer of text from the external legacy encoding into Unicode.
\end{description}

Note that although at the time of writing, \XeTeX\ relies on TEC for encoding conversion of input text,
this may change in a future release.
A future version will probably use either ICU or GNU {\em libiconv} functions instead of TEC.
This would be in the interests of portability to operating systems other than Mac~OS~X.

It is also possible that the input mapping support will be extended to allow the use of SIL's TECkit\footnote{Text Encoding Conversion toolkit; see \url{http://scripts.sil.org/teckit}.} to directly support custom user-defined byte encodings. This would involve a minor extension to the \verb|\XeTeXinputencoding| command, allowing users to specify the name of a TECkit mapping file as an alternative to the name of a standard legacy codepage.

\subsection{Font mappings using TECkit}

Widely-used \TeX\ keyboarding conventions such as \verb|\'{e}| $\mapsto$ ‘é’ or \verb|\pounds| $\mapsto$ ‘\pounds’ are implemented via \TeX\ macros (and therefore easily adapted for Unicode-compliant fonts, by modifying the macro definitions).
In addition, there are a few established conventions that are implemented as ligature rules associated with standard \TeX\ fonts; these include \verb|---| $\mapsto$ ‘---’ (em-dash), \verb|?`| $\mapsto$ ‘?`’, and a few more.
In principle, smart font technologies such as AAT and OpenType could implement these same ligatures, providing the same behavior as traditional \TeX\ fonts. But as these conventions are peculiar to the \TeX\ world, it is not realistic to expect them to be provided in mainstream, general-purpose fonts.

Although it would usually be possible to simulate these ligatures via macro programming, it is difficult to ensure that reprogramming widely-used text characters such as the hyphen, question mark, and quotation marks will not interfere with other levels of markup in the source document.
Instead, \XeTeX\ provides a mechanism known as “font mappings”, whereby a mapping of Unicode characters is associated with a particular font, and applied to all strings of text being measured or rendered in that font. This is implemented using the TECkit mapping engine.

Although primarily designed to convert between legacy byte encodings and Unicode, TECkit can also be used to perform transformations on a Unicode text stream, using the same mapping language; figure~\ref{fig-tex-text} shows the {\em tex-text} mapping that is provided to support normal \TeX\ conventions.
When associated with a standard Unicode-compliant font in \XeTeX, this has the effect of implementing the legacy \TeX\ conventions for dashes and quotes, as shown in figure~\ref{fig-mapping}, without requiring any \TeX-specific features in the smart fonts themselves.

\begin{figure}[tb]
\hrule\smallskip
\small
\begin{verbatim}
; TECkit mapping for TeX input conventions <-> Unicode characters
LHSName	"TeX-text"
RHSName	"UNICODE"

pass(Unicode)
U+002D U+002D        <> U+2013 ; -- -> en dash
U+002D U+002D U+002D <> U+2014 ; --- -> em dash

U+0027               <> U+2019 ; ' -> right single quote
U+0027 U+0027        <> U+201D ; '' -> right double quote
U+0022                > U+201D ; " -> right double quote

U+0060               <> U+2018 ; ` -> left single quote
U+0060 U+0060        <> U+201C ; `` -> left double quote

U+0021 U+0060        <> U+00A1 ; !` -> inverted exclam
U+003F U+0060        <> U+00BF ; ?` -> inverted question
\end{verbatim}\smallskip\hrule
\caption{The TECkit source file {\tt tex-text.map}, defining a font mapping for \XeTeX\
that provides compatibility with the conventions of legacy \TeX\ fonts.}
\label{fig-tex-text}
\end{figure}

\begin{figure}[tb]
\hrule\smallskip
\begin{minipage}{0.6\hsize}
\begin{verbatim}
\font\TestA="Times New Roman" at 9pt
\TestA !`Typing ``quotes''---and
  dashes---the \TeX\ way!\par
\bigskip
\font\TestB="Times New Roman:
  mapping=tex-text" at 9pt
\TestB !`Typing ``quotes''---and
  dashes---the \TeX\ way!\par
\end{verbatim}
\end{minipage}\hfil
\begin{minipage}{0.4\hsize}
\font\TestA="Times New Roman" at 9pt
\TestA !`Typing ``quotes''---and
  dashes---the \TeX\ way!\par
\bigskip
\font\TestB="Times New Roman:
  mapping=tex-text" at 9pt
\TestB !`Typing ``quotes''---and
  dashes---the \TeX\ way!\par
\end{minipage}
\smallskip\hrule
\caption{Using the {\em tex-text} font mapping to support legacy typing conventions.}
\label{fig-mapping}
\end{figure}

While this mechanism, associating a mapping defined in terms of Unicode character sequences, was first devised in order to support legacy \TeX\ input conventions, it can also be applied in other ways. For example, figure~\ref{fig-translit} shows how it is possible to typeset a single fragment of input text in two scripts by giving different font specifications, one of which includes a transliteration mapping.

\begin{figure}[tb]
\hrule\smallskip
\begin{minipage}{0.5\hsize}
\begin{verbatim}
\begin{centering}
\def\SampleText{Unicode - это уникальный
   код для любого символа,\\
    независимо от платформы,\\
    независимо от программы,\\
    независимо от языка.\par}
\font\gen="Gentium" at 9pt \gen
\SampleText
\bigskip
\font\gentrans="Gentium:
  mapping=cyr-lat-iso9" at 9pt \gentrans
\SampleText
\end{centering}
\end{verbatim}
\end{minipage}\hfil
\begin{minipage}{0.5\hsize}
\def\SampleText{Unicode - это уникальный
   код для любого символа,\\
    независимо от платформы,\\
    независимо от программы,\\
    независимо от языка.\par}
\begin{centering}
\font\gen="Gentium" at 9pt \gen
\SampleText
\bigskip
\font\gentrans="Gentium:
  mapping=cyr-lat-iso9" at 9pt \gentrans
\SampleText
\end{centering}
\end{minipage}
\smallskip\hrule
\caption{Using a font mapping for on-the-fly transliteration while typesetting.}
\label{fig-translit}
\end{figure}

\subsection{Math typesetting}

One of \TeX's traditional strengths is in mathematical typesetting. It was designed to enable authors to readily typeset complex equations and similar displays, with precise control over details of layout and spacing, and with many aspects of math formatting handled automatically. Figure~\ref{fig-math} shows an example of \TeX\ input using math mode, alongside the typeset result.

\begin{figure}[tb]
\footnotesize
\hrule\smallskip
\begin{minipage}{0.55\hsize}
\begin{verbatim}
$$\eqalign{
\biggl(\int_{-\infty}^\infty e^{-x^2}\,dx\biggr)^2
&=\int_{-\infty}^\infty\int_{-\infty}^\infty
  e^{-(x^2+y^2)}\,dx\,dy\cr
&=\int_0^{2\pi}\int_0^\infty e^{-r^2}r\,dr\,d\theta\cr
&=\int_0^{2\pi}\biggl(-{e^{-r^2}\over2}
  \bigg\vert_{r=0}^{r=\infty}\,\biggr)\,d\theta\cr
&=\pi.\cr}$$
\end{verbatim}
\end{minipage}\hfil
\begin{minipage}{0.45\hsize}
\catcode`@=11     % make @ like a letter
\newdimen\jot \jot=3pt
\newskip\z@skip \z@skip=0pt plus0pt minus0pt
\newdimen\z@ \z@=0pt % can be used both for 0pt and 0
\dimendef\dimen@=0
\def\m@th{\mathsurround=\z@}
\def\ialign{\everycr{}\tabskip\z@skip\halign} % initialized \halign
\def\openup{\afterassignment\@penup\dimen@=}
\def\@penup{\advance\lineskip\dimen@
  \advance\baselineskip\dimen@
  \advance\lineskiplimit\dimen@}
\def\eqalign#1{\null\,\vcenter{\openup\jot\m@th
  \ialign{\strut\hfil$\displaystyle{##}$&$\displaystyle{{}##}$\hfil
      \crcr#1\crcr}}\,}
\catcode`@=12   % back to nonletter category
\catcode`\_=8
$$\eqalign{
 \biggl(\int_{-\infty}^\infty e^{-x^2}\,dx\biggr)^2
  &=\int_{-\infty}^\infty\int_{-\infty}^\infty
    e^{-(x^2+y^2)}\,dx\,dy\cr
  &=\int_0^{2\pi}\int_0^\infty e^{-r^2}r\,dr\,d\theta\cr
  &=\int_0^{2\pi}\biggl(-{e^{-r^2}\over2}
    \bigg\vert_{r=0}^{r=\infty}\,\biggr)\,d\theta\cr
  &=\pi.\cr}$$
\end{minipage}
\smallskip\hrule
\caption{An example of math typesetting, one of \TeX's strengths.}
\label{fig-math}
\end{figure}

Standard \TeX\ macro packages provide commands to access many hundreds of math and other technical symbols, and these are mapped to the appropriate characters in a selection of specialized fonts.
In Unicode many such symbols now have their own codepoints, and so it should be possible to typeset such material using standard Unicode-compliant fonts, rather than the custom-encoded math and symbol fonts normally used with \TeX.

However, this is not as simple as it may sound. \TeX's math typesetting features rely heavily on specialized font metric information associated with the math fonts, in addition to the glyphs themselves. This is necessary in order to provide accurate typesetting of complex constructs. Unfortunately, this means that users cannot simply tell \XeTeX\ to use a Unicode-compliant font for math; not only will the standard \TeX\ math commands access the wrong glyphs, but also, even if the \TeX\ macros were redefined to access the proper Unicode values, such fonts will still not work in math mode because of the lack of extended font metric information.

To address this issue, it will be necessary to provide these additional font metrics for any Unicode fonts that are to be used for math typesetting; and it will also be necessary to extend additional data structures in \XeTeX\ that are currently limited to 8-bit codes. It may be possible to make use of work from the Omega project (see section~\ref{sect-omega}) to facilitate this, but at the time of writing, \XeTeX\ is limited to using legacy 8-bit \TeX\ fonts for math typesetting.

\section{Advanced font features}

In order to take full advantage of the multilingual support and sophisticated typographic features offered by modern font technologies, we need to go beyond merely specifying a font and rendering a sequence of Unicode characters.
Correct rendering may depend on the language of the text, as different languages sometimes require different visual results even for the exact same Unicode characters.

For example, Turkish uses both of the characters {\em i} (U+0069 {\sc latin small letter i}) and {\em ı} (U+0131 {\sc latin small letter dotless i}). But many Latin-script fonts include an {\em fi} ligature in which the dot of the {\em i} is assimilated into the top of the {\em f}.
Such a ligature is appropriate for most languages, and improves the appearance of the rendered text;
and typical OpenType or AAT fonts will contain ligature rules that automatically use it wherever the sequence {\tt fi} occurs in the text stream.
However, in Turkish it becomes a problem because the distinction between {\em i} and {\em ı} is lost. Figure~\ref{fig-turkish-bad} illustrates this issue.\footnote{Sample Turkish text from the Unicode web site, \url{http://www.unicode.org/standard/translations/turkish.html}.}

\def\red{\aftergroup\resetcolor \special{color push rgb 0.5 0.0 0.0}} \def\resetcolor{\special{color pop}}
\begin{figure}[hb]
\hrule\smallskip
\small
\begin{minipage}{0.55\hsize}
\begin{verbatim}
\font\txtfont="Adobe Garamond Pro" at 8pt
\txtfont
Apple, HP, IBM, JustSystem, Microsoft, Oracle,
SAP, Sun, Sybase, Unisys ve endüstrinin diğer
ileri gelen {\red firmaları} Evrensel Kod
Standardını desteklemektedirler. Evrensel Kod,
XML, Java, ECMAScript (JavaScript), LDAP, CORBA
3.0, WML vb. gibi modern standartlar
{\red tarafından} ISO/IEC 10646 uyarlanmasının
resmi yoludur.
\end{verbatim}
\end{minipage}\hfil
\begin{minipage}{0.45\hsize}
\font\txtfont="Adobe Garamond Pro" at 8pt
\txtfont
Apple, HP, IBM, JustSystem, Microsoft, Oracle, SAP, Sun, Sybase, Unisys ve endüstrinin  diğer ileri gelen {\red firmaları}  Evrensel Kod Standardını desteklemektedirler. Evrensel Kod, XML, Java, ECMAScript (JavaScript), LDAP, CORBA 3.0, WML vb. gibi modern standartlar {\red tarafından} ISO/IEC 10646 uyarlanmasının  resmi yoludur.
\end{minipage}
\smallskip\hrule
\caption{Turkish text showing the necessity for language-specific font rendering. Note how the automatic use of the {\em fi} ligature obscures the distinction between {\em i} and {\em ı}, which are separate letters in Turkish.}
\label{fig-turkish-bad}
\end{figure}

An increasing number of OpenType fonts provide a solution to this problem, in the form of support for multiple “language systems”. The font developer can provide tables within the font that substitute different glyphs, enable different subsets of the possible ligatures, etc., depending on the selected language.
\XeTeX\ supports this capability by allowing a font specification in the \TeX\ document to include a language tag. If the specified language tag is supported by the font, \XeTeX\ will render the font according to those OpenType rules instead of the default behavior. Figure~\ref{fig-turkish-good} shows how the Turkish example can be corrected by adding the proper language tag to the font declaration.

\begin{figure}
\hrule\smallskip
\small
\begin{minipage}{0.55\hsize}
\begin{verbatim}
\font\txtfont="Adobe Garamond Pro:
  language=TUR" at 8pt
\txtfont
Apple, HP, IBM, JustSystem, Microsoft, Oracle,
SAP, Sun, Sybase, Unisys ve endüstrinin diğer
ileri gelen {\red firmaları} Evrensel Kod
Standardını desteklemektedirler. Evrensel Kod,
XML, Java, ECMAScript (JavaScript), LDAP, CORBA
3.0, WML vb. gibi modern standartlar
{\red tarafından} ISO/IEC 10646 uyarlanmasının
resmi yoludur.
\end{verbatim}
\end{minipage}\hfil
\begin{minipage}{0.45\hsize}
\def\red{\aftergroup\resetcolor \special{color push rgb 0.5 0.0 0.0}} \def\resetcolor{\special{color pop}}
\font\txtfont="Adobe Garamond Pro:
  language=TUR" at 8pt
\txtfont
Apple, HP, IBM, JustSystem, Microsoft, Oracle, SAP, Sun, Sybase, Unisys ve endüstrinin  diğer ileri gelen {\red firmaları}  Evrensel Kod Standardını desteklemektedirler. Evrensel Kod, XML, Java, ECMAScript (JavaScript), LDAP, CORBA 3.0, WML vb. gibi modern standartlar {\red tarafından} ISO/IEC 10646 uyarlanmasının  resmi yoludur.
\end{minipage}
\smallskip\hrule
\caption{Using the Turkish language support in an OpenType font to ensure correct rendering of {\tt fi} sequences.}
\label{fig-turkish-good}
\end{figure}

\font\doulosA="Doulos SIL/AAT:Uppercase Eng alternates=Large eng with descender" at 10pt
\font\doulosB="Doulos SIL/AAT:Uppercase Eng alternates=Large eng on baseline" at 10pt
\font\doulosC="Doulos SIL/AAT:Uppercase Eng alternates=Large eng with short stem" at 10pt
\font\doulosD="Doulos SIL/AAT:Uppercase Eng alternates=Capital N with tail" at 10pt

Another issue is that some Unicode characters have alternative possible glyph shapes (within a single typeface design). For example, the character U+014A {\sc latin capital letter eng} has four possible designs in the Doulos SIL typeface: {\doulosA Ŋ \doulosB Ŋ \doulosC Ŋ \doulosD Ŋ}. These are all legitimate renderings of the same Unicode character, the uppercase version of {\em eng} ({\doulosA ŋ}). By default, text rendered in Doulos SIL will use the first of these forms. But in some language communities, a different form may be preferred—or may even be required for readability.

One solution to this would be to use different language systems within the OpenType tables to access the alternate {\em Eng} glyphs. In practice, however, this is more difficult to arrange than the Turkish case. This character is used in many lesser-known and little-documented languages, and so font developers cannot reasonably be expected to provide all the appropriate language system mappings.

An alternative approach is through user-selectable {\em font features}. These are another mechanism for controlling exactly how a given font renders text. Rather than associating variant glyphs or other options with a particular language, the options can be made available as individual “features” that can be enabled or disabled as required. So the Doulos SIL font, for example, has a feature named {\em Uppercase Eng alternates}, with four possible settings, and the character U+014A can be displayed as any of the four available glyphs according to the feature setting chosen.

\XeTeX\ allows such features to be included as part of the font specification used to load a particular font for typesetting. Figure~\ref{fig-eng} shows how the same text can be typeset either with the default glyphs provided in Doulos SIL, or using alternate forms for particular characters. This mechanism enables the user to achieve culturally-appropriate rendering of the Unicode text without requiring the font developer to be aware of the proper choices of glyph variants for each language where the font may be used.

\begin{figure}
\hrule\smallskip
\begin{minipage}{0.67\hsize}
\begin{verbatim}
\font\Doulos="Doulos SIL/AAT" at 10pt
\font\DoulosAlt="Doulos SIL/AAT:
  Alternate forms=Literacy alternates,
    Small v-hook straight style;
  Uppercase Eng alternates=Capital N with tail"
    at 10pt
\Doulos
Xɔsee na Mose ɖo Ŋutitotoŋkeke la anyi, eye wòna
wohlẽ ʋu ɖe ʋɔtrutiwo ŋu bene dɔla si atsrɔ̃
ŋgɔgbeviwo la nagawɔ nuvevi Israel viwo ya o.\par
\bigskip
\DoulosAlt
Xɔsee na Mose ɖo Ŋutitotoŋkeke la anyi, eye wòna
wohlẽ ʋu ɖe ʋɔtrutiwo ŋu bene dɔla si atsrɔ̃
ŋgɔgbeviwo la nagawɔ nuvevi Israel viwo ya o.\par
\end{verbatim}
\end{minipage}\hfil
\begin{minipage}{0.33\hsize}
\baselineskip=12pt \lineskiplimit=-100pt
\rightskip=0pt plus 1fil
\font\Doulos="Doulos SIL/AAT"
    at 10pt
\font\DoulosAlt="Doulos SIL/AAT:
  Alternate forms=Literacy alternates,
    Small v-hook straight style;
  Uppercase Eng alternates=Capital N with tail"
    at 10pt
{
\let\c=\char \let\p=\par \let\bs=\bigskip
\catcode`\ʋ=\active \defʋ{{\red\c`\ʋ}}
\catcode`\Ŋ=\active \defŊ{{\red\c`\Ŋ}}
\catcode`\g=\active \defg{{\red\c`\g}}
\catcode`\a=\active \defa{{\red\c`\a}}
\Doulos
Xɔsee na Mose ɖo Ŋutitotoŋkeke la anyi,
eye wòna wohlẽ ʋu ɖe ʋɔtrutiwo ŋu bene
dɔla si atsrɔ̃ ŋgɔgbeviwo la nagawɔ nuvevi
Israel viwo ya o.\p
\bs
\DoulosAlt
Xɔsee na Mose ɖo Ŋutitotoŋkeke la anyi,
eye wòna wohlẽ ʋu ɖe ʋɔtrutiwo ŋu bene
dɔla si atsrɔ̃ ŋgɔgbeviwo la nagawɔ nuvevi
Israel viwo ya o.\p
}
\end{minipage}
\smallskip\hrule
\caption{Using optional font features to control the rendering of specific Unicode characters. This may be for stylistic reasons, or may be required for readability in a particular language community.}
\label{fig-eng}
\end{figure}

\section{\XeTeX\ and other \TeX\ extensions}

The \XeTeX\ project is just one of a number of extended versions of \TeX\ that have been created over the years, and it may be useful to give a brief comment on its relationship to some other projects.

\subsection{\TeXgX}

\TeXgX\footnote{See \url{http://www.sil.org/computing/texgx.html}.} is an important ancestor of \XeTeX, in that it pioneered the model of integrating the \TeX\ formatting system with a host platform's smart-font rendering technology. \TeXgX\ relied on Apple's now-obsolete QuickDraw GX graphics system for Mac OS 7–9, and was still based on 8-bit legacy encodings, not Unicode. However, it did adapt \TeX's scanning and paragraphing routines to treat entire words as units to be passed to an external text layout system, as well as extending the \verb|\font| command to load fonts from the host platform and to access optional features. These extensions have been incorporated largely unchanged into \XeTeX.

\subsection{\eTeX}

\eTeX\footnote{See \url{http://www.tug.org/tex-archive/systems/e-tex/v2/}.} is a widely-used extended version of \TeX\ that adds a number of new commands to the language, while retaining compatibility with the standard program. It is in fact used by many installations as the default ‘\TeX’ program, and an increasing number of macro packages assume that \eTeX\ features are available.

\XeTeX\ is implemented as an extension of \eTeX, so that it benefits from the enhancements provided in that system. In particular, support for bidirectional paragraph layout, needed for languages such as Arabic and Hebrew, is inherited from \eTeX.

\subsection{Omega, Aleph}
\label{sect-omega}

Omega\footnote{See \url{http://omega.enstb.org/} and \url{http://www.tex.ac.uk/cgi-bin/texfaq2html?label=omega}.} is an ambitious project that extends \TeX\ to work with 16-bit character codes and provides a mechanism of input and output filters (“Omega transformation processes”). These can perform complex transformations both as text is read from a file (to support different encodings) and between the internal character codes and font access codes.

Aleph\footnote{See \url{http://www.tex.ac.uk/cgi-bin/texfaq2html?label=aleph}.}, formerly known as e-Omega, is a project that aims to merge features of Omega and \eTeX, and to provide a more stable platform than Omega, which has been undergoing major restructuring and appears to face a somewhat uncertain future.

Omega (and therefore Aleph) provides a very powerful mechanism for supporting multilingual typesetting and complex scripts. However, because (like \TeX\ itself) it is intended to be platform-independent, it does not take advantage of available text systems such as ATSUI; instead, all complex script behavior must be implemented through Omega's own OTP mechanisms. This is both a strength and a weakness: a strength in that the mechanism is both powerful and portable; but a weakness in that configuring fonts (especially for complex scripts) to work with Omega is a non-trivial programming task, beyond the capabilities of many users.

In contrast, one of the key ideas of \XeTeX\ is that it should {\em not} be necessary to re-implement complex script and typographic behavior that has already been defined by the developers of AAT and OpenType fonts. If a user's computer system supports a given writing system, with appropriate fonts and rendering behavior, this should be immediately usable in the typesetting system; no laborious, technical configuration procedure should be needed. This gives \XeTeX\ a major ease-of-use advantage, but comes at a price: the typesetting system is now dependent on the host platform's font technology, in a way that standard \TeX\ is not, and so documents may not be readily portable to other platforms.

While \XeTeX\ does not share Omega's complex-script features, taking an entirely different approach to text rendering, it does use ideas from Omega with regard to the “widening” of character codes within the \TeX\ engine from 8 to 16 bits.

\subsection{pdf\TeX}

One more \TeX\ extension is worth mentioning here: pdf\TeX\ is an extended version of \TeX\ that provides the option to generate PDF output, instead of the traditional DVI that requires post-processing with a device-specific driver. It includes a number of additional commands to enhance the resulting PDF files with bookmarks, interactivity, and so on.

While \XeTeX\ also generates PDF output, this is actually accomplished by generating “extended DVI” output, and then rendering this to PDF as a subsequent process. This is less efficient than pdf\TeX's direct PDF generation, and makes it more difficult to integrate some of the advanced PDF features (although to some extent, this can still be done through the traditional DVI-to-device driver model).

At present, then, \XeTeX\ is entirely separate from pdf\TeX. Integration of \XeTeX's Unicode and font support with pdf\TeX's PDF generation and extended PDF features would provide an even more attractive typesetting system, but would require significant development effort; at the time of writing, no such effort is under way.

\section{Future directions}

This paper has discussed how Unicode and multilingual/multi-script support has been integrated into the \TeX\ system, providing users with a powerful typesetting system that handles virtually any language for which an appropriate “smart font” is available. However, \XeTeX\ should still be considered somewhat experimental, a “work in progress”, and there is considerable scope for further enhancement. A few possible directions for further work include:

\begin{description}

\item[Graphite support] Another smart font technology, besides AAT and OpenType, is SIL International's Graphite\footnote{See \url{http://scripts.sil.org/RenderingGraphite}.} system. This could be integrated as a third text rendering option within \XeTeX.

\item[Alternate platforms] Currently, \XeTeX\ is available only on Mac OS X. It was initially developed on this platform by taking advantage of specific Mac OS X technologies (especially ATSUI for Unicode text layout, and Quartz2D for graphics/PDF rendering). However, there is considerable interest in porting to other operating systems.

\item[pdf\TeX\ integration] As suggested above, it would seem ideal to merge \XeTeX's Unicode support with pdf\TeX's PDF generation.

\item[Unicode math font support] This has also been mentioned above; currently, only legacy 8-bit fonts can be used for math typesetting. Fully supporting Unicode for math will probably require coordination among font developers and \TeX\ macro writers, as well as extensions to the \XeTeX\ system itself.

\item[Line-breaking without spaces] Writing systems that do not use spaces between words are currently not well supported in \XeTeX. Line-breaking and paragraph layout relies on recognizing potential line break positions, primarily at spaces, and so will not work in languages such as Thai or Chinese. It is possible to work around this by use of zero-width spaces in the source text, but ideally the paragraphing algorithm should be extended to handle such writing systems correctly.

\end{description}

While these are offered as examples of how \XeTeX, or perhaps some future system modeled on the current project, might develop further, this does {\em not} constitute a commitment to implement any particular feature!

\end{document}
